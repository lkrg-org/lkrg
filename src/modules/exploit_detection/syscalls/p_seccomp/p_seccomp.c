/*
 * pi3's Linux kernel Runtime Guard
 *
 * Component:
 *  - Intercept SECCOMP policy update
 *
 * Notes:
 *  - Process SECCOMP Exploit Detection validation
 *
 * Caveats:
 *  - None
 *
 * Timeline:
 *  - Created: 18.XI.2017
 *
 * Author:
 *  - Adam 'pi3' Zabrocki (http://pi3.com.pl)
 *
 */

#include "../../../../p_lkrg_main.h"

#if defined(P_LKRG_EXPLOIT_DETECTION_SECCOMP_H)
char p_seccomp_kretprobe_state = 0;

static struct kretprobe p_seccomp_kretprobe = {
#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,17,0)
    .kp.symbol_name = "do_seccomp",
#else
    .kp.symbol_name = "prctl_set_seccomp",
#endif
    .handler = p_seccomp_ret,
    .entry_handler = p_seccomp_entry,
    .data_size = sizeof(struct p_seccomp_data),
};

#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,17,0)
char p_get_seccomp_filter_kretprobe_state = 0;

static struct kretprobe p_get_seccomp_filter_kretprobe = {
    .kp.symbol_name = "get_seccomp_filter",
    .handler = p_get_seccomp_filter_ret,
    .entry_handler = p_get_seccomp_filter_entry
};
GENERATE_INSTALL_FUNC(get_seccomp_filter)

/*
 * get_seccomp_filter() is a global function that is called from
 * seccomp_sync_threads(). Since seccomp_sync_threads() is responsible for
 * synchronizing the current task's seccomp filter across all threads when
 * SECCOMP_FILTER_FLAG_TSYNC is specified, we can use get_seccomp_filter() to
 * know when threads are about to be synced. This is significant because the
 * get_seccomp_filter() call inside seccomp_sync_threads() is the only thing we
 * can probe in the sighand-locked critical section seccomp_set_mode_filter()
 * uses to synchronize seccomp filter updates. The sighand lock prevents new
 * threads from being added to the group's thread list, which is the only way to
 * ensure that a new task won't slip through the cracks and miss our
 * p_set_ed_process_off(). This could otherwise happen AFTER p_seccomp_entry()
 * but BEFORE seccomp_set_mode_filter() acquires the sighand lock, resulting in
 * a stale seccomp shadow for the task that slipped through the cracks.
 */
int p_get_seccomp_filter_entry(struct kretprobe_instance *p_ri, struct pt_regs *p_regs) {

   struct task_struct *thread;
   struct p_ed_process *p_tmp;
   bool sync_threads = false;

   /*
    * Determine if this get_seccomp_filter() call is the first call from the
    * loop in seccomp_sync_threads(). flag_tsync_done is used to track if this
    * is the first time get_seccomp_filter() is called in the loop.
    */
   if ((p_tmp = ed_task_lock_current())) {
      if (p_tmp->p_ed_task.p_sec.flag_in_seccomp &&
          !p_tmp->p_ed_task.p_sec.flag_tsync_done) {
         p_tmp->p_ed_task.p_sec.flag_tsync_done = 1;
         sync_threads = true;
      }
      ed_task_unlock(p_tmp);
   }

   if (sync_threads) {
      /*
       * SECCOMP_FILTER_FLAG_TSYNC is guaranteed to succeed at this point, so
       * prepare for all threads in the group to receive the seccomp filter.
       * Threads that have PF_EXITING set are skipped as an optimization since
       * seccomp_sync_threads() will just skip them too.
       *
       * for_each_thread() is stable under the sighand lock, so an RCU read lock
       * isn't needed here.
       */
      for_each_thread(current, thread) {
         if (thread == current || (thread->flags & PF_EXITING))
            continue;

         /* RCU read lock is only needed for actually looking up the task */
         rcu_read_lock();
         p_tmp = ed_task_find_lock_rcu(thread);
         rcu_read_unlock();
         if (p_tmp) {
            /*
             * flag_sync_thread may have been set from a previous thread-sync
             * seccomp() that is in-flight and has yet to finish. In that case,
             * leave flag_sync_thread set and don't set the off flag again since
             * the off flag is already set.
             */
            if (!p_tmp->p_ed_task.p_sec.flag_sync_thread) {
               p_tmp->p_ed_task.p_sec.flag_sync_thread = 1;
               p_set_ed_process_off(p_tmp);
            }
            ed_task_unlock(p_tmp);
         }
      }
   }

   return 0;
}

int p_get_seccomp_filter_ret(struct kretprobe_instance *p_ri, struct pt_regs *p_regs) {
   return 0;
}

/* Must be called with sighand lock held */
static void p_flush_filter_tsync(void) {

   struct p_ed_process *p_tmp;
   struct task_struct *thread;

   /*
    * It's possible for there to be a race between an indirect seccomp filter
    * update due to SECCOMP_FILTER_FLAG_TSYNC and then a subsequent update
    * either directly from the task itself or another SECCOMP_FILTER_FLAG_TSYNC.
    * This can happen because flag_sync_thread is cleared outside of the
    * sighand-locked critical section where the seccomp filter thread sync takes
    * place (which is unavoidable due to probing limitations detailed in
    * p_get_seccomp_filter_entry()). As a result, this task could end up with a
    * stale seccomp shadow.
    *
    * To mitigate this, we acquire the sighand lock in order to know when the
    * last seccomp thread sync is truly complete, at which point we can clear
    * flag_sync_thread for all threads early.
    */
   for_each_thread(current, thread) {
      /* RCU read lock is only needed for actually looking up the task */
      rcu_read_lock();
      p_tmp = ed_task_find_lock_rcu(thread);
      rcu_read_unlock();
      if (p_tmp) {
         if (p_tmp->p_ed_task.p_sec.flag_sync_thread) {
            /*
             * Clear flag_sync_thread and update the task's seccomp shadow. The
             * update can be skipped for the current task as an optimization
             * since the current task will either get updated anyway in
             * p_seccomp_ret() or get its off-flag set in p_seccomp_entry()
             * because it might be installing a new filter.
             */
            p_tmp->p_ed_task.p_sec.flag_sync_thread = 0;
            if (thread != current)
               p_update_ed_process(p_tmp, thread, 0);
            p_set_ed_process_on(p_tmp);
         }
         ed_task_unlock(p_tmp);
      }
   }
}
#endif

/*
 * x86-64 syscall ABI:
 *  *rax - syscall_number
 *    rdi - 1st argument
 *    rsi - 2nd argument
 *    rdx - 3rd argument
 *    rcx - 4th argument
 *
 *    r8  - 5th one
 *    r9  - 6th one
 */

int p_seccomp_entry(struct kretprobe_instance *p_ri, struct pt_regs *p_regs) {

   bool positive_ret_ok = false;
   struct p_ed_process *p_tmp;
#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,17,0)
   unsigned long flags;

#if LINUX_VERSION_CODE >= KERNEL_VERSION(5,0,0)
   positive_ret_ok = p_regs_get_arg2(p_regs) & SECCOMP_FILTER_FLAG_NEW_LISTENER;
#endif
   spin_lock_irqsave(&current->sighand->siglock, flags);
   p_flush_filter_tsync();
#endif
   if ((p_tmp = ed_task_lock_current())) {
      p_set_ed_process_off(p_tmp);
      p_tmp->p_ed_task.p_sec.flag_in_seccomp = 1;
      p_tmp->p_ed_task.p_sec.flag_tsync_done = 0;
      p_tmp->p_ed_task.p_sec.flag_positive_ret_ok = positive_ret_ok;
      ed_task_unlock(p_tmp);
   }
#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,17,0)
   spin_unlock_irqrestore(&current->sighand->siglock, flags);
#endif
   return 0;
}


int p_seccomp_ret(struct kretprobe_instance *p_ri, struct pt_regs *p_regs) {

   const long ret = p_regs_get_ret(p_regs);
   struct p_ed_process *p_tmp;
#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,17,0)
   unsigned long flags;

   spin_lock_irqsave(&current->sighand->siglock, flags);
   p_flush_filter_tsync();
#endif
   if ((p_tmp = ed_task_lock_current())) {
      p_tmp->p_ed_task.p_sec.flag_in_seccomp = 0;
      /* Update the current task's shadow only if seccomp() succeeded */
      if (!ret || (ret > 0 && p_tmp->p_ed_task.p_sec.flag_positive_ret_ok))
         p_update_ed_process(p_tmp, current, 0);
      p_set_ed_process_on(p_tmp);
      ed_task_unlock(p_tmp);
   }
#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,17,0)
   spin_unlock_irqrestore(&current->sighand->siglock, flags);
#endif
   return 0;
}


GENERATE_INSTALL_FUNC(seccomp)
#endif
